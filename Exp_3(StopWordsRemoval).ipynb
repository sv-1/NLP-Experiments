{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtxnlZ0nk2Oo",
        "outputId": "869d8bf1-ab4b-4999-cb08-1776418590bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK\n",
        "!pip install nltk\n",
        "\n",
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEYLj2Wvk_cu",
        "outputId": "e0ee0a8f-44e1-42cd-910d-7c8fe445d77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "\n",
        "# Tokenization using word_tokenize\n",
        "tokens = word_tokenize(text)"
      ],
      "metadata": {
        "id": "AL1ufDKXlEFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Porter Stemmer and WordNet Lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "s5TkzMQ8lGrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stemming and lemmatization\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]"
      ],
      "metadata": {
        "id": "vHJYsoGslKlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_stemmed_tokens = [token for token in stemmed_tokens if token.lower() not in stop_words]\n",
        "filtered_lemmatized_tokens = [token for token in lemmatized_tokens if token.lower() not in stop_words]\n"
      ],
      "metadata": {
        "id": "LBk1yTDllOgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print results\n",
        "print(\"Original Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\nStemmed Tokens (with stop words):\")\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZEaOUDTlR9O",
        "outputId": "05a62d14-683c-4166-e8d3-b9942802b25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens:\n",
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
            "\n",
            "Stemmed Tokens (with stop words):\n",
            "['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'human', 'languag', 'data', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStemmed Tokens (without stop words):\")\n",
        "print(filtered_stemmed_tokens)\n",
        "\n",
        "print(\"\\nLemmatized Tokens (with stop words):\")\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "-zST13LXlWVN",
        "outputId": "87fc77e2-9fa1-4df4-ae61-29c93d4bed5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemmed Tokens (without stop words):\n",
            "['nltk', 'lead', 'platform', 'build', 'python', 'program', 'work', 'human', 'languag', 'data', '.']\n",
            "\n",
            "Lemmatized Tokens (with stop words):\n",
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'program', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLemmatized Tokens (without stop words):\")\n",
        "print(filtered_lemmatized_tokens)"
      ],
      "metadata": {
        "id": "EDKIrhhjlaud",
        "outputId": "ff537ff2-0cd7-4616-b6e9-2531b5efbe80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized Tokens (without stop words):\n",
            "['NLTK', 'leading', 'platform', 'building', 'Python', 'program', 'work', 'human', 'language', 'data', '.']\n"
          ]
        }
      ]
    }
  ]
}